{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7055634",
   "metadata": {},
   "source": [
    "# Analyse du fonds Daniel Thurre\n",
    "\n",
    "\n",
    "## 1. Importation des bibliothèques logicielles et des routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour Section 3\n",
    "#%pip install geopandas\n",
    "#%pip install deep-translator\n",
    "#%pip install geopy\n",
    "#%pip install highlight-text\n",
    "\n",
    "# Pour Section 4\n",
    "#!pip3 install keras\n",
    "#!pip3 install tensorflow\n",
    "\n",
    "# Pour Section 5\n",
    "#%pip install retina-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab32a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gestion des données\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b259d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# production des figures\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from collections import Counter\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "from highlight_text import fig_text\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "\n",
    "matplotlib.rc('font',family='Times New Roman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce459dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traduction \n",
    "from deep_translator import GoogleTranslator\n",
    "from deep_translator import DeeplTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eabb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partionnement des images\n",
    "\n",
    "# chargement et analyse des images  \n",
    "from keras.preprocessing.image import load_img \n",
    "from keras.preprocessing.image import img_to_array \n",
    "from keras.applications.vgg16 import preprocess_input \n",
    "\n",
    "# models \n",
    "from keras.applications.vgg16 import VGG16 \n",
    "from keras.models import Model\n",
    "\n",
    "# partitionnement et reduction dimensionelle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconnaissance faciale \n",
    "\n",
    "from retinaface import RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reste (os, math, date)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "today = datetime.date.today()\n",
    "print(\"Dernière mise à jour : \", today)\n",
    "\n",
    "from random import randint\n",
    "import pickle\n",
    "import cv2\n",
    "import requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd63cf4",
   "metadata": {},
   "source": [
    "## 2. Importation des données et création du tableur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71889a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76508145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin vers le dossier contenant les images\n",
    "path = '/Users/aureliavalterio/Desktop/Mémoire_HN/OK_VF/'\n",
    "\n",
    "# lecture du dossier et création de la liste\n",
    "dossier = os.listdir(path)\n",
    "\n",
    "# estimation du nombre d'image dans la liste \n",
    "# il se peut qu'un ou deux fichiers autres que des images se soient glissés dans la liste\n",
    "# ces dernier seront mis de côtés lors de l'analyse\n",
    "print('Le nombre d\\'images est',len(dossier),'.')\n",
    "dossier[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe164a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de la liste pour enregistrer le dictionnaire \n",
    "liste_dic = []\n",
    "\n",
    "# sélection des fichiers avec des extensions images, définition des extensions\n",
    "ext = [\".jpeg\", \".png\", \".jpg\"] \n",
    "\n",
    "# création du dictionnaire\n",
    "for files in dossier:\n",
    "    if files.endswith(tuple(ext)): # sélection des fichiers avec des extensions images\n",
    "        file, ext = os.path.splitext(files) # séparation des données pour chaque images\n",
    "        file_info = file.split(\"_\")\n",
    "#        print(file_info) # Décommenter si message d'erreur pour corriger liste des titres \n",
    "        dic = {\n",
    "            \"numéro\": file_info[0],\n",
    "            \"type\": file_info[1],\n",
    "            \"dénomination\": file_info[2],\n",
    "            \"siècle\": file_info[3],\n",
    "            \"ville_prod\": file_info[4],\n",
    "            \"pays_prod\": file_info[5],\n",
    "            \"ville_cons\": file_info[6],\n",
    "            \"pays_cons\": file_info[7],\n",
    "            \"commentaire\": file_info[8],\n",
    "        }\n",
    "        liste_dic.append(dic)\n",
    "\n",
    "# Impression du dictionnaire\n",
    "print(liste_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(liste_dic)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704c5e7",
   "metadata": {},
   "source": [
    "# 3 Description du corpus de données\n",
    "\n",
    "### 3.1 Analyse des types présents dans le fonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4adf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['type']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325bef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Répartition des types présents dans le fonds\n",
    "Type = df1['type'].value_counts()\n",
    "Type = Type.to_frame().reset_index()\n",
    "Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3794d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un nouveau tableur Types top + reste\n",
    "Type_top_plus_rest = df1['type'].value_counts()\n",
    "Type_top_plus_rest = Type_top_plus_rest.to_frame().reset_index()\n",
    "Type_top_plus_rest\n",
    "\n",
    "# Nous gardons les types apparaissant plus de 10 fois dans le fonds, c'est à dire les 8 premiers.\n",
    "start_row = 8\n",
    "\n",
    "# Compte de tous les autres types et regroupement dans une nouvelle catégorie 'Autres'.\n",
    "Type_top_plus_rest.iloc[start_row] = Type_top_plus_rest.iloc[start_row:].sum()\n",
    "Type_top_plus_rest = Type_top_plus_rest.drop(start_row+1)\n",
    "Type_top_plus_rest = Type_top_plus_rest.iloc[:start_row+1]\n",
    "\n",
    "\n",
    "# Nouvelles entêtes des colonnes.\n",
    "Type_top_plus_rest.columns = ['Type', 'Nombre d\\'objets']\n",
    "\n",
    "# Nommer la nouvelle catégorie 'Autres'.\n",
    "Type_top_plus_rest.loc[start_row,'Type']='Autres'\n",
    "\n",
    "\n",
    "Type_top_plus_rest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagramme circulaire\n",
    "\n",
    "# création de la figure\n",
    "plt.figure(figsize = (10, 10))\n",
    "\n",
    "# choix de la donnée à représenter, ici le nombre d'objets\n",
    "x = Type_top_plus_rest[\"Nombre d'objets\"]\n",
    "\n",
    "# création du diagramme\n",
    "plt.pie(x, \n",
    "        labels = Type_top_plus_rest[\"Type\"], # choix de l'étiquette, ici les noms des types\n",
    "#        explode =  (0, 0.2),\n",
    "        autopct = lambda x: str(round(x, 2)) + r'%', # affichage des %'s arrondis à une décimale\n",
    "        pctdistance = 0.8, \n",
    "        labeldistance = 1.1, # distance entre le diagramme et les étiquettes\n",
    "        shadow = False, # ombrage oui/non\n",
    "        colors = ['#d73027','#f46d43','#fdae61','#fee090','#ffffbf','#e0f3f8','#abd9e9','#74add1','#4575b4'], # couleurs\n",
    "        wedgeprops={\"alpha\": 0.8}, # opacité\n",
    "       textprops={'fontsize': 20}, # taille du texte\n",
    "       startangle=90) # choix de l'angle de départ, ici, on représente la catégorie des reliquaires (~50%) sur la gauche du diagramme\n",
    "#plt.legend(loc=1,fontsize= 20) # pas de légence\n",
    "plt.text(-.8, -1.2, 'Auteure :  Valterio Aurelia', ha='right', va='bottom') # pied-de-page \n",
    "plt.title(\"Distribution des types\", fontsize=25,y=.95) # Titre\n",
    "\n",
    "plt.savefig('Figs/Types_camembert.pdf',bbox_inches=\"tight\") # Sauvegarde du diagramme dans un dossier Figs.\n",
    "plt.show() # Affichage du diagramme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagramme diagramme en barres\n",
    "\n",
    "# Création du diagramme (type, couleur et opacité des barres, couleur et opacité des contours, taille de la figure)\n",
    "ax = Type_top_plus_rest.set_index(\"Type\").plot(kind = \"bar\", figsize=(10,5),\n",
    "      color=([[0.7, 0, 0, .3]]),edgecolor=([[0.7, 0, 0, 1.]]))\n",
    "\n",
    "# Étiquettes des axes\n",
    "ax.set_xlabel(\"Type\", fontsize=15) # étiquette de l'axe des abscisses (x)\n",
    "ax.set_ylabel('Nombre d\\'objets', fontsize=15) # étiquette de l'axe des ordonnées (y)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12) # choix des ticks\n",
    "\n",
    "# Titre\n",
    "ax.set_title(\"Distribution des types\", fontsize=25,y=1.01) \n",
    "\n",
    "# Légende \n",
    "ax.get_legend().remove() # supression de la légende     \n",
    "#ax.legend(fontsize=12)\n",
    "\n",
    "# pied de page\n",
    "ax.text(0.115, -0.2, 'Auteure :  Valterio Aurelia',transform=plt.gcf().transFigure)\n",
    "\n",
    "# enregistrement de l'image\n",
    "plt.savefig('Figs/Types_bars.pdf',bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fcfc6a",
   "metadata": {},
   "source": [
    "### 3.2 Analyse chronologiques des objets présents dans le fonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['siècle']] # création du nouveau tableur\n",
    "df1= df1.replace('NaN', np.nan) # remplacement des entrées 'NaN' (string) par np.nan (numpy float)\n",
    "df1.dropna(inplace=True) # élimination des entrées nan.\n",
    "df1.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.siècle.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ef2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sépare la colonne siècle avec la commande split et les \":\" dans deux nouvelles colonnes siècle_min et siècle_max\n",
    "df1[['siècle_min', 'siècle_max']] = df1['siècle'].str.rsplit(\":\",n=1, expand=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d381b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle for pour remplacer les None par les valeurs de la colonne siècle_min.\n",
    "i=0 # indice de la ligne\n",
    "col_index = df1.columns.get_loc('siècle_max') # indice de la colonne siècle_max\n",
    "for item in df1.siècle.str.split(\":\"): # itération sur toutes les lignes du tableur\n",
    "    if len(item)==1: # si la longueur de item est 1 (un seul siècle, pas de séparation)\n",
    "        if item[0]=='NaN':  # on vérifie que l'entrée est bien un siècle et non un NaN\n",
    "            df1.iloc[i,col_index] = 'NaN'\n",
    "        else:\n",
    "            df1.iloc[i,col_index] = str(int(item[0])) # on copie l'entrée dans la colonne siècle_max\n",
    "    i+=1\n",
    "    \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableur occurences siècle estimation basse\n",
    "Siècle_estimation_basse = df1['siècle_min'].value_counts()\n",
    "Siècle_estimation_basse = Siècle_estimation_basse.to_frame().reset_index()\n",
    "Siècle_estimation_basse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ed432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableur occurences siècle estimation haute\n",
    "Siècle_estimation_haute = df1['siècle_max'].value_counts()\n",
    "Siècle_estimation_haute = Siècle_estimation_haute.to_frame().reset_index()\n",
    "Siècle_estimation_haute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df76b87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# récupération des données en float\n",
    "s_b = Siècle_estimation_basse['siècle_min'].to_numpy().astype(float) # siècles estimation basse\n",
    "count_b = Siècle_estimation_basse['count'].to_numpy() # nombre d'occurences siècles estimation basse\n",
    "s_m = Siècle_estimation_haute['siècle_max'].to_numpy().astype(float) # siècles estimation haute\n",
    "count_m = Siècle_estimation_haute['count'].to_numpy() # nombre d'occurences siècles estimation haute\n",
    "\n",
    "# création de la figure\n",
    "f, ax = plt.subplots(figsize=(10,5),facecolor='w',edgecolor='k')\n",
    "\n",
    "# largeur des barres\n",
    "width = 0.3\n",
    "\n",
    "# diagramme en barres, estimation basse, décalage de width/2 vers la gauche, largeur des barres width \n",
    "# couleur des barres et du cadre RGB [0.7, 0, 0] opacité des barres 0.3, opacité des cadres 1.0\n",
    "ax.bar(s_b-width/2, count_b,width,color=([[0.7, 0, 0, .3]]),edgecolor=([[0.7, 0, 0, 1.]]),label='Estimation basse')\n",
    "\n",
    "# diagramme en barres, estimation basse, décalage de width/2 vers la droite, largeur des barres width \n",
    "# couleur des barres et du cadre RGB [0.17, 0.55, 0.74] opacité des barres 0.3, opacité des cadres 1.0\n",
    "ax.bar(s_m+width/2, count_m,width,color=([[0.17, 0.55, 0.74, .3]]),edgecolor=([[0.17, 0.55, 0.74, 1.]]),label='Estimation haute')\n",
    "\n",
    "# Étiquette axe des x\n",
    "ax.set_xlabel(\"Siècle\", fontsize=15) \n",
    "# Étiquette axe des y\n",
    "ax.set_ylabel(\"Nombre d'objets\", fontsize=15) \n",
    "# Ticks\n",
    "ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Titre\n",
    "ax.set_title(\"Répartition chronologique\", fontsize=25,y=1.01) \n",
    "\n",
    "# Légende, loc=2 (en haut à gauche)\n",
    "ax.legend(fontsize=15,loc=2)\n",
    "\n",
    "# Pied-de-page en bas à gauche\n",
    "ax.text(0.115, -0.05, 'Auteure : Valterio Aurelia',transform=plt.gcf().transFigure)\n",
    "\n",
    "# Sauvegarde de la figure dans le dossier Figs\n",
    "plt.savefig('Figs/Siècles.pdf',bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c60ffd",
   "metadata": {},
   "source": [
    "### 3.3 Analyse chronologique et par type des objets présents dans le fonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Même commandes que précédemment pour crééer la dataframe avec les siècles min et max\n",
    "df1 = df[['siècle','type']]\n",
    "df1= df1.replace('NaN', np.nan)\n",
    "df1.dropna(inplace=True)\n",
    "df1[['siècle_min', 'siècle_max']] = df1['siècle'].str.rsplit(\":\",n=1, expand=True)\n",
    "i=0\n",
    "col_index = df1.columns.get_loc('siècle_max')\n",
    "for item in df1.siècle.str.split(\":\"):\n",
    "    if len(item)==1:\n",
    "        if item[0]=='NaN':\n",
    "            df1.iloc[i,col_index] = 'NaN'\n",
    "        else:\n",
    "            df1.iloc[i,col_index] = str(int(item[0]))\n",
    "#    if len(item)==3:\n",
    "#        print(item)\n",
    "    i+=1\n",
    "    \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2c110",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On crée une nouvelle colonne new_type\n",
    "df1['new_type'] = df1['type']\n",
    "\n",
    "# Répartition des types présents dans le fonds afin d'identifier les quatres plus fréquents\n",
    "Type = df1['type'].value_counts()\n",
    "Type = Type.to_frame().reset_index()\n",
    "\n",
    "Type.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d098087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1['new_type'].value_counts()[df1['type']].values < Type.iloc[3, 1], 'new_type'] = \"Autre\"\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa53021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['new_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2337d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[['siècle_min','new_type']]\n",
    "df2['count']=1\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb49fad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.pivot_table(\n",
    "    index=['siècle_min'],\n",
    "    columns='new_type',\n",
    "    values='count',aggfunc='sum').reset_index().fillna(0)\n",
    "df3['siècle_min'] = df3['siècle_min'].astype(float)\n",
    "df3.sort_values(\"siècle_min\",inplace=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du diagramme en barres, on choisit stacked=True afin d'avoir la fragmentation par type\n",
    "# les couleurs pour les 5 catégories sont choisies suivant la convention RBG et l'opacité alpha : [R,G,B,alpha].\n",
    "ax = df3.sort_values(\"siècle_min\").plot.bar(x='siècle_min', stacked=True,  figsize=(10,5), \n",
    "                                            color=([[0.84, 0.1, 0.1, .8],[0.99, 0.68, 0.38, .8],[1, 1, 0.74, .8],\n",
    "                                                    [0.67, 0.85, 0.91, .8],[0.17, 0.48, 0.71, .8]]))\n",
    "\n",
    "# Étiquette axe des x\n",
    "ax.set_xlabel(\"Siècle\", fontsize=15) \n",
    "# Étiquette axe des x\n",
    "ax.set_ylabel(\"Nombre d'objets\", fontsize=15) \n",
    "\n",
    "\n",
    "# Ticks\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Axe des x, on soit faire apparaitre le siècle au format numérique mais sans décimales après la virgule.\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "i=0\n",
    "for item in labels:\n",
    "    item = item.split('.')[0]\n",
    "    labels[i]=item\n",
    "    i+=1\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "# Titre du graphique\n",
    "ax.set_title(\"Répartition chronologique par type\", fontsize=25,y=1.01) \n",
    "\n",
    "\n",
    "# Légende     \n",
    "ax.legend(fontsize=15)\n",
    "\n",
    "# Pied-de-page    \n",
    "ax.text(0.1, -0.0, 'Auteure : Valterio Aurelia',transform=plt.gcf().transFigure)\n",
    "\n",
    "# Sauvegarde dans le dossier Figs\n",
    "plt.savefig('Figs/Siècles_vs_types.pdf',bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1dbc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "types_array = np.array([Type.iloc[0,0],Type.iloc[1,0],Type.iloc[2,0],Type.iloc[3,0],'Autre'])\n",
    "types_array\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    ax = df3.sort_values(\"siècle_min\").plot.bar(x='siècle_min',y=types_array[i],figsize=(10,5),color=([[0.7, 0, 0, .3]]),edgecolor=([[0.7, 0, 0, 1.]]))\n",
    "\n",
    "    ax.set_xlabel(\"Siècle\", fontsize=16) #titre de l'axe des abscisses\n",
    "    ax.set_ylabel(\"Nombre d'objets\", fontsize=16) #titre de l'axe des abscisses\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    j=0\n",
    "    for item in labels:\n",
    "        item = item.split('.')[0]\n",
    "        labels[j]=item\n",
    "        j+=1\n",
    "\n",
    "    ax.set_xticklabels(labels)\n",
    "\n",
    "    ax.set_title(\"Répartition chronologique pour le type \\\"\"+types_array[i]+\"\\\"\", fontsize=20) \n",
    "    ax.text(0.1, -0.0, 'Auteure : Valterio Aurelia',transform=plt.gcf().transFigure)\n",
    "\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "    plt.savefig('Figs/Siècles_vs_'+types_array[i]+'.pdf',bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e880c2",
   "metadata": {},
   "source": [
    "### 3.4 Analyse géographique\n",
    "\n",
    "#### 3.4.1 Répartition par pays de production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4cf497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['pays_prod']]\n",
    "df1= df1.replace('NaN', np.nan) # Elimination des entrées Nan\n",
    "df1.dropna(inplace=True)        # Elimination des entrées Nan\n",
    "df1[\"pays_prod\"] = df1[\"pays_prod\"].str.split(\":\") # Séparation des entrées avec plusieurs pays de production.\n",
    "df1 = df1.explode(\"pays_prod\").reset_index(drop=True) # Création de nouvelles lignes avec un pays par ligne.\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75987f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries = df1['pays_prod'].value_counts()\n",
    "Countries = Countries.to_frame().reset_index()\n",
    "Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traduction des noms de pays en anglais\n",
    "Countries['country_prod'] = Countries['pays_prod'] # Copie des noms de pays dans une nouvelle colonne à traduire.\n",
    "\n",
    "# Boucle de traduction\n",
    "i=0\n",
    "col_index = Countries.columns.get_loc('country_prod')\n",
    "col_index\n",
    "for item in Countries.pays_prod:\n",
    "    Countries.iloc[i,col_index] = GoogleTranslator(source='fr', target='en').translate(item) # Traduction de la ligne en englais\n",
    "    print(item,GoogleTranslator(source='fr', target='en').translate(item) ) # Impression du nom en anglais afin de vérifier la traduction\n",
    "    i+=1\n",
    "\n",
    "Countries_en = Countries[['country_prod','count']]\n",
    "Countries_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d59d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries_en.loc[Countries_en.country_prod=='England', 'country_prod'] = 'United Kingdom'\n",
    "Countries_en.loc[Countries_en.country_prod=='Scotland', 'country_prod'] = 'United Kingdom'\n",
    "Countries_en.loc[Countries_en.country_prod=='Sicily', 'country_prod'] = 'Italy'\n",
    "Countries_en.loc[Countries_en.country_prod=='Swiss', 'country_prod'] = 'Switzerland'\n",
    "Countries_en.loc[Countries_en.country_prod=='Türkiye', 'country_prod'] = 'Turkey'\n",
    "Countries_en.loc[Countries_en.country_prod=='Suede', 'country_prod'] = 'Sweden'\n",
    "Countries_en.loc[Countries_en.country_prod=='Czech Republic', 'country_prod'] = 'Czechia'\n",
    "Countries_en.loc[Countries_en.country_prod=='Czechian', 'country_prod'] = 'Czechia'\n",
    "Countries_en.loc[Countries_en.country_prod=='The Netherlands', 'country_prod'] = 'Netherlands'\n",
    "Countries_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9286677",
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries_en = Countries_en.groupby([\"country_prod\"]).agg({\"count\":\"sum\"}).reset_index()\n",
    "Countries_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation du fichier avec les coordonnées géographiques des pays du monde\n",
    "world = gpd.read_file(\"/Users/aureliavalterio/Desktop/Mémoire_HN/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\")\n",
    "\n",
    "# croisement entre le tableur world et le tableur country_en\n",
    "data = world.merge(Countries_en, how='left',\n",
    "                    left_on='NAME', right_on='country_prod')\n",
    "# supression des pays qui n'ont pas produit d'objets\n",
    "data.dropna(subset=['count'], inplace=True)\n",
    "\n",
    "# replacement du nombre d'occurences par un pourcentage\n",
    "data['count'] = round(data['count']/data['count'].sum()*100,1)\n",
    "\n",
    "# vérification\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34853af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de la figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# choix des couleurs de colorisation des pays\n",
    "cmap = cm.Oranges # tons orangés\n",
    "min_rate, max_rate = -0, 20 # échelle de colorization, dégradé entre 0 et 20%\n",
    "norm = mcolors.Normalize(vmin=min_rate, vmax=max_rate) # Normalisation de l'échelle\n",
    "\n",
    "\n",
    "# représentation des données, pays colorisés suivant le nombre d'objets produits\n",
    "data.plot(column='count', cmap=cmap, norm=norm,\n",
    "          edgecolor='black', linewidth=0.2,alpha=0.85, ax=ax)\n",
    "\n",
    "# représentation des frontières en noir (et des pays non producteurs en gris clair)\n",
    "world.plot(\n",
    "    ax=ax,\n",
    "    color=(\"lightgrey\"),\n",
    "    edgecolor=\"black\",\n",
    "    alpha=.05\n",
    ")\n",
    "\n",
    "# Choix du domaine représenté (ici Europe + Moyen-Orient)\n",
    "ax.set_xlim(-10, 65)\n",
    "ax.set_ylim(20, 70)\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "## Annotation textuelles\n",
    "# extraction de la position des centres des pays.\n",
    "data_projected = data.to_crs(epsg=3035)\n",
    "data_projected['centroid'] = data_projected.geometry.centroid\n",
    "data['centroid'] = data_projected['centroid'].to_crs(data.crs)\n",
    "# liste des pays à annoter\n",
    "countries_to_annotate = ['France', 'Italy', 'Switzerland','Georgia','Austria',\n",
    "                         'Syria','Sweden','Czechia','Netherlands',\n",
    "                         'Belgium', 'Germany','United Kingdom','Turkey','Iran','Egypt','Spain'] \n",
    "\n",
    "# Ajustement manuel de la position des texte 'Nom du pays':(dx,dy) où dx et dy sont les variation horizontales et verticales de la position\n",
    "adjustments = {\n",
    "    'France': (9, 3),\n",
    "    'Italy': (-3, 1.5),\n",
    "    'Lithuania': (0, -0.6),\n",
    "    'Finland': (0, -2.5),\n",
    "    'Romania': (0, -0.5),\n",
    "    'Bulgaria': (0, -0.6),\n",
    "    'Greece': (-1.2, -0.8),\n",
    "    'Croatia': (0, -1),\n",
    "    'Cyprus': (0, -1),\n",
    "    'Ireland': (0, -1),\n",
    "    'Malta': (0, -1),\n",
    "    'Slovenia': (0, -1),\n",
    "    'Slovakia': (-0.7, -0.8),\n",
    "    'Estonia': (0, -0.7),\n",
    "    'Latvia': (0, -0.5),\n",
    "    'Belgium': (0, -0.7),\n",
    "    'Austria': (1, -1),\n",
    "    'Spain': (0, -1),\n",
    "    'Portugal': (-0.5, -1),\n",
    "    'Luxembourg': (0, -1),\n",
    "    'Germany': (-0.2, 0),\n",
    "    'Hungary': (-0.3, -1),\n",
    "    'Czechia': (0, -1),\n",
    "    'Poland': (0, -1),\n",
    "    'Sweden': (-1.5, -1),\n",
    "    'Denmark': (0, -1),\n",
    "    'Netherlands': (-1.5,-0.5),\n",
    "    'United Kingdom': (0, -2),\n",
    "    'Switzerland': (0, -1),\n",
    "    'Turkey':(0, -1.),\n",
    "    'Iran':(0, 0),\n",
    "    'Egypt':(-1, -1),\n",
    "    'Georgia':(0, -1),\n",
    "    'Czechia':(0, -1),\n",
    "    'Syria':(-0.5, -0.8),\n",
    "    'Sweden':(-1.5, 0),\n",
    "}\n",
    "\n",
    "# annotation\n",
    "for country in countries_to_annotate:\n",
    "\n",
    "    # position du texte\n",
    "    centroid = data.loc[data['NAME'] == country, 'centroid'].values[0]\n",
    "    x, y = centroid.coords[0]\n",
    "\n",
    "    # correction de la position\n",
    "    x += adjustments[country][0]\n",
    "    y += adjustments[country][1]\n",
    "\n",
    "    # texte\n",
    "    code = data.loc[data['NAME'] == country, 'SOV_A3'].values[0]\n",
    "    rate = data.loc[data['NAME'] == country, 'count'].values[0]\n",
    "    ax.annotate(f'{code[:2]} {rate}%', (x, y), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=10, fontfamily='DejaVu Sans', color='black')\n",
    "\n",
    "\n",
    "# Titre\n",
    "ax.set_title(\"Répartition géographique des lieux d'origine des objets\", fontsize=25,y=1.01) \n",
    "\n",
    "\n",
    "# pied-de-page\n",
    "text = \"<Auteure>: Aurelia Valterio\\n<Source>: https://www.naturalearthdata.com/\"\n",
    "fig_text(0.075, 0.23,\n",
    "         s=text,\n",
    "         color='black',\n",
    "         fontsize=9,\n",
    "         highlight_textprops=[{\"fontweight\": 'bold'},\n",
    "                              {\"fontweight\": 'bold'}],\n",
    "         ax=ax)\n",
    "\n",
    "\n",
    "# création de la figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figs/Geo_pays_prod_full.pdf',bbox_inches=\"tight\") # sauvegarde dans le dossier Figs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de la figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# choix des couleurs de colorisation des pays\n",
    "cmap = cm.Oranges # tons orangés\n",
    "min_rate, max_rate = -0, 30 # échelle de colorization, dégradé entre 0 et 30%\n",
    "norm = mcolors.Normalize(vmin=min_rate, vmax=max_rate) # Normalisation de l'échelle\n",
    "\n",
    "\n",
    "\n",
    "# représentation des données, pays colorisés suivant le nombre d'objets produits\n",
    "data.plot(column='count', cmap=cmap, norm=norm,\n",
    "          edgecolor='black', linewidth=0.2,alpha=0.85, ax=ax)\n",
    "\n",
    "# représentation des frontières en noir (et des pays non producteurs en gris clair)\n",
    "world.plot(\n",
    "    ax=ax,\n",
    "    color=(\"lightgrey\"),\n",
    "    edgecolor=\"black\",\n",
    "    alpha=.05\n",
    ")\n",
    "\n",
    "# Choix du domaine représenté (ici Europe)\n",
    "ax.set_xlim(-10, 32)\n",
    "ax.set_ylim(32, 70)\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "## Annotation textuelles\n",
    "# extraction de la position des centres des pays\n",
    "data_projected = data.to_crs(epsg=3035)\n",
    "data_projected['centroid'] = data_projected.geometry.centroid\n",
    "data['centroid'] = data_projected['centroid'].to_crs(data.crs)\n",
    "# liste des pays à annoter\n",
    "countries_to_annotate = ['France', 'Italy', 'Switzerland','Georgia','Austria',\n",
    "                         'Syria','Sweden','Czechia','Netherlands',\n",
    "                         'Belgium', 'Germany','United Kingdom','Turkey','Iran','Egypt','Spain']\n",
    "adjustments = {\n",
    "    'France': (10, 3),\n",
    "    'Italy': (-0.5, -0),\n",
    "    'Lithuania': (0, -0.6),\n",
    "    'Finland': (0, -2.5),\n",
    "    'Romania': (0, -0.5),\n",
    "    'Bulgaria': (0, -0.6),\n",
    "    'Greece': (-1.3, -0.),\n",
    "    'Croatia': (0, -1),\n",
    "    'Cyprus': (0, -1),\n",
    "    'Ireland': (0, -1),\n",
    "    'Malta': (0, -1),\n",
    "    'Slovenia': (0, -1),\n",
    "    'Slovakia': (-0.7, -0.8),\n",
    "    'Estonia': (0, -0.7),\n",
    "    'Latvia': (0, -0.5),\n",
    "    'Belgium': (-0.1, -0.5),\n",
    "    'Austria': (0.2, -0.5),\n",
    "    'Spain': (0, -1),\n",
    "    'Portugal': (-0.5, -1),\n",
    "    'Luxembourg': (0, -1),\n",
    "    'Germany': (-0.2, 0),\n",
    "    'Hungary': (-0.3, -1),\n",
    "    'Czechia': (-0.5, -0.5),\n",
    "    'Poland': (0, -1),\n",
    "    'Sweden': (-1.5, -1),\n",
    "    'Denmark': (-0.8, -0.1),\n",
    "    'Netherlands': (-0.2, -0.3),\n",
    "    'United Kingdom': (1, -2),\n",
    "    'Switzerland': (-0.2, -0.5),\n",
    "    'Turkey':(-6,-1.),\n",
    "    'Iran':(0, 0),\n",
    "    'Egypt':(0, 0),\n",
    "    'United States of America':(0, 0),\n",
    "    'Sweden':(-1, 0),\n",
    "    'Georgia':(0, 0),\n",
    "    'Egypt':(-1, -1),\n",
    "    'Syria':(-0.5, -0.8),\n",
    "}\n",
    "\n",
    "\n",
    "# annotation\n",
    "for country in countries_to_annotate:\n",
    "\n",
    "    # position du texte\n",
    "    centroid = data.loc[data['NAME'] == country, 'centroid'].values[0]\n",
    "    x, y = centroid.coords[0]\n",
    "\n",
    "    # correction de la position\n",
    "    x += adjustments[country][0]\n",
    "    y += adjustments[country][1]\n",
    "\n",
    "    # texte\n",
    "    code = data.loc[data['NAME'] == country, 'SOV_A3'].values[0]\n",
    "    rate = data.loc[data['NAME'] == country, 'count'].values[0]\n",
    "    ax.annotate(f'{code[:2]} {rate}%', (x, y), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=10, fontfamily='DejaVu Sans', color='black')\n",
    "\n",
    "# Titre\n",
    "ax.set_title(\"Répartition géographique des lieux d'origine des objets\", fontsize=25,y=1.01) \n",
    "\n",
    "# Annotation des pays extra-européens\n",
    "ax.annotate('Hors-carte :', (-5, 65), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=15,fontweight='bold', color='black')\n",
    "rateEgypt = data.loc[data['NAME'] == 'Egypt', 'count'].values[0]\n",
    "ax.annotate('Égypte %1.1f' %rateEgypt+'%', (-5, 64), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=15, color='black')\n",
    "\n",
    "rateGeorgia = data.loc[data['NAME'] == 'Georgia', 'count'].values[0]\n",
    "ax.annotate('Géorgie %1.1f' %rateGeorgia+'%', (-5, 63), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=15, color='black')\n",
    "\n",
    "\n",
    "rateSyria = data.loc[data['NAME'] == 'Syria', 'count'].values[0]\n",
    "ax.annotate('Syrie %1.1f' %rateSyria+'%', (-5, 62), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=15, color='black')\n",
    "rateIran = data.loc[data['NAME'] == 'Iran', 'count'].values[0]\n",
    "ax.annotate('Iran %1.1f' %rateIran+'%', (-5, 61), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=15, color='black')\n",
    "\n",
    "\n",
    "# pied-de-page\n",
    "text = \"<Auteure>: Aurelia Valterio\\n<Source>: https://www.naturalearthdata.com/\"\n",
    "fig_text(0.085, .1,\n",
    "         s=text,\n",
    "         color='black',\n",
    "         fontsize=9,\n",
    "         highlight_textprops=[{\"fontweight\": 'bold'},\n",
    "                              {\"fontweight\": 'bold'}],\n",
    "         ax=ax)\n",
    "\n",
    "\n",
    "\n",
    "# création de la figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figs/Geo_pays_prod.pdf',bbox_inches=\"tight\") # sauvegarde dans le dossier Figs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec247068",
   "metadata": {},
   "source": [
    "#### 3.4.2 Répartition par ville de production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db46ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['ville_prod']]\n",
    "df1= df1.replace('NaN', np.nan) # Elimination des NaN\n",
    "df1.dropna(inplace=True)        # Elimination des NaN\n",
    "df1[\"ville_prod\"] = df1[\"ville_prod\"].str.split(\":\")  # Séparation des entréees qui contiennent plusieurs villes de production.\n",
    "df1 = df1.explode(\"ville_prod\").reset_index(drop=True) # Éclatement du tableur afin d'avoir une ville par ligne\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8cfba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Villes_prod = df1['ville_prod'].value_counts()\n",
    "Villes_prod = Villes_prod.to_frame().reset_index()\n",
    "Villes_prod.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cab21b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Affiner à la main la dataframe\n",
    "Villes_prod.loc[Villes_prod.ville_prod=='Saint Maurice', 'ville_prod'] = 'Saint Maurice Suisse'\n",
    "Villes_prod.loc[Villes_prod.ville_prod=='Cologne', 'ville_prod'] = 'Cologne Allemagne'\n",
    "Villes_prod.loc[Villes_prod.ville_prod=='Bâle', 'ville_prod'] = 'Bâle Suisse'\n",
    "Villes_prod.loc[Villes_prod.ville_prod=='Trèves', 'ville_prod'] = 'Trèves Allemagne'\n",
    "Villes_prod.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'API Nominatim\n",
    "geolocator = Nominatim(user_agent=\"MyApp\")\n",
    "\n",
    "location = geolocator.geocode(\"Aywiers\")\n",
    "\n",
    "print(\"The latitude of the location is: \", location.latitude)\n",
    "print(\"The longitude of the location is: \", location.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e369c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialisation de l'API Nominatim\n",
    "geolocator = Nominatim(user_agent=\"MyApp\")\n",
    "\n",
    "# Définition d'une fonction qui calcule les coordonnées (latitude et longitude) d'une ville\n",
    "def coordinates(city):\n",
    "    location = geolocator.geocode(city)\n",
    "    latitude = 'NaN'\n",
    "    longitude = 'NaN'\n",
    "    if location !=None:\n",
    "        latitude=location.latitude\n",
    "        longitude=location.longitude\n",
    "    return latitude, longitude\n",
    "\n",
    "# Création de deux colonnes vides (latitude et longitude)\n",
    "Villes_prod[['latitude','longitude']] = ['NaN','NaN']\n",
    "\n",
    "# Boucle for pour entrer les latitudes et les longitudes des villes avec la fonction coordinates\n",
    "i=0\n",
    "col_index_lat = Villes_prod.columns.get_loc('latitude') # indice de la colonne latitude \n",
    "col_index_long = Villes_prod.columns.get_loc('longitude') # indice de la colonne longitude \n",
    "for item in Villes_prod.ville_prod:\n",
    "    print(item) # impression du nom de la ville\n",
    "    location = coordinates(item)  # calcul des coordonnées avec la fonction coordinates\n",
    "    Villes_prod.iloc[i,col_index_lat]=location[0] # enregistrement de la latitude\n",
    "    Villes_prod.iloc[i,col_index_long]=location[1]  # enregistrement de la longitude\n",
    "    time.sleep(1.1) # pause d'1.1 second, car l'API est limité à 1 requête par seconde.\n",
    "    i+=1\n",
    "\n",
    "Villes_prod= Villes_prod.replace('NaN', np.nan) # Elimination des NaN\n",
    "Villes_prod.dropna(inplace=True)                # Elimination des NaN\n",
    "Villes_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14254920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Villes_prod.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a64e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates('Cologne Allemagne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941843a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les données géographiques du monde\n",
    "world = gpd.read_file(\"/Users/aureliavalterio/Desktop/Mémoire_HN/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\")\n",
    "\n",
    "# Création de la figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Création de l'arrière plan : les pays colorisés légèrement\n",
    "world.plot(ax=ax,color='#fdcc8a',alpha=0.2,edgecolor='None')\n",
    "\n",
    "# Représentation des données: disque coloré sur chaque ville productrice, la taille dépend du nombre d'objets\n",
    "sc1 = ax.scatter(x=Villes_prod['longitude'], y=Villes_prod['latitude'],\n",
    "           s=(Villes_prod['count']/Villes_prod['count'].sum())*2000,\n",
    "          facecolors='#b30000',alpha=0.3, edgecolors='#b30000')\n",
    "# Représentation des données: cercle coloré sur chaque ville productrice, la taille dépend du nombre d'objets\n",
    "sc1 = ax.scatter(x=Villes_prod['longitude'], y=Villes_prod['latitude'],\n",
    "           s=(Villes_prod['count']/Villes_prod['count'].sum())*2000,\n",
    "          facecolors='None',alpha=1, edgecolors='#b30000')\n",
    "\n",
    "# Avant plan : les frontières colorisés légèrement\n",
    "world.plot(ax=ax,color='None',alpha=.05,edgecolor='black')\n",
    "\n",
    "# Choix du domaine représenté (ici Europe, même limites que précédemment)\n",
    "ax.set_xlim(-10, 32)\n",
    "ax.set_ylim(32, 70)\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "## Création de la légende\n",
    "# marqueur pour 1 objet\n",
    "one = mlines.Line2D([], [], marker='o', linestyle='None',color='#b30000',\n",
    "                          markersize=(1/Villes_prod['count'].sum())*600,markerfacecolor=([0.70,0,0,0.5]),markeredgecolor=([0.70,0,0,1]), label='1')\n",
    "# marqueur pour 5 objets\n",
    "five = mlines.Line2D([], [], marker='o', linestyle='None',color='#b30000',\n",
    "                          markersize=(5/Villes_prod['count'].sum())*200,markerfacecolor=([0.70,0,0,0.5]),markeredgecolor=([0.70,0,0,1]), label='5')\n",
    "# marqueur pour 10 objets\n",
    "ten = mlines.Line2D([], [], marker='o', linestyle='None',color='#b30000',\n",
    "                          markersize=(10/Villes_prod['count'].sum())*200,markerfacecolor=([0.70,0,0,0.5]),markeredgecolor=([0.70,0,0,1]), label='10')\n",
    "# marqueur pour 20 objets\n",
    "twenty = mlines.Line2D([], [], marker='o', linestyle='None',color='#b30000',\n",
    "                          markersize=(20/Villes_prod['count'].sum())*140,markerfacecolor=([0.70,0,0,0.5]),markeredgecolor=([0.70,0,0,1]), label='20')\n",
    "# affichage de la légende\n",
    "plt.legend(handles=[one, five, ten,twenty],loc=2,fontsize=20)\n",
    "\n",
    "# Titre\n",
    "ax.set_title(\"Répartition géographique des villes de production\", fontsize=25,y=1.01) \n",
    "\n",
    "# pied-de-page\n",
    "text = \"<Auteure>: Aurelia Valterio\\n<Source>: https://www.naturalearthdata.com/\"\n",
    "fig_text(0.085, .1,\n",
    "         s=text,\n",
    "         color='black',\n",
    "         fontsize=9,\n",
    "         highlight_textprops=[{\"fontweight\": 'bold'},\n",
    "                              {\"fontweight\": 'bold'}],\n",
    "         ax=ax)\n",
    "\n",
    "# affichage et sauvegarde de la figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figs/Geo_villes_origine.pdf',bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da3f8b",
   "metadata": {},
   "source": [
    "#### 3.4.3 Répartition par pays de conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['pays_cons']]\n",
    "df1= df1.replace('NaN', np.nan)\n",
    "df1.dropna(inplace=True)\n",
    "df1[\"pays_cons\"] = df1[\"pays_cons\"].str.split(\":\")\n",
    "df1 = df1.explode(\"pays_cons\").reset_index(drop=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fcf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des types présents dans le fonds\n",
    "Countries = df1['pays_cons'].value_counts()\n",
    "Countries = Countries.to_frame().reset_index()\n",
    "Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c665f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries['country_cons'] = Countries['pays_cons']\n",
    "i=0\n",
    "col_index = Countries.columns.get_loc('country_cons')\n",
    "col_index\n",
    "for item in Countries.pays_cons:\n",
    "    Countries.iloc[i,col_index] = GoogleTranslator(source='fr', target='en').translate(item) \n",
    "    print(item,GoogleTranslator(source='fr', target='en').translate(item) )\n",
    "    i+=1\n",
    "\n",
    "Countries_en = Countries[['country_cons','count']]\n",
    "Countries_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13906740",
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries_en.loc[Countries_en.country_cons=='Suede', 'country_cons'] = 'Sweden'\n",
    "Countries_en.loc[Countries_en.country_cons=='England', 'country_cons'] = 'United Kingdom'\n",
    "Countries_en.loc[Countries_en.country_cons=='Scotland', 'country_cons'] = 'United Kingdom'\n",
    "Countries_en.loc[Countries_en.country_cons=='Swiss', 'country_cons'] = 'Switzerland'\n",
    "Countries_en.loc[Countries_en.country_cons=='USA', 'country_cons'] = 'United States of America'\n",
    "Countries_en.loc[Countries_en.country_cons=='The Netherlands', 'country_cons'] = 'Netherlands'\n",
    "Countries_en.loc[Countries_en.country_cons=='Czech Republic', 'country_cons'] = 'Czechia'\n",
    "Countries_en.loc[Countries_en.country_cons=='Czechian', 'country_cons'] = 'Czechia'\n",
    "Countries_en.loc[Countries_en.country_cons=='Suede', 'country_cons'] = 'Sweden'\n",
    "\n",
    "\n",
    "Countries_en = Countries_en.groupby([\"country_cons\"]).agg({\"count\":\"sum\"}).reset_index()\n",
    "Countries_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(\"/Users/aureliavalterio/Desktop/Mémoire_HN/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\")\n",
    "\n",
    "data = world.merge(Countries_en, how='left',\n",
    "                    left_on='NAME', right_on='country_cons')\n",
    "data.dropna(subset=['count'], inplace=True)\n",
    "data['count'] = round(data['count']/data['count'].sum()*100,1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7533e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création de la figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# choix des couleurs\n",
    "cmap = cm.Oranges # tons orangés\n",
    "min_rate, max_rate = -0, 30 # échelle de colorization, dégradé entre 0 et 30%\n",
    "norm = mcolors.Normalize(vmin=min_rate, vmax=max_rate) # Normalisation de l'échelle\n",
    "\n",
    "\n",
    "# représentation des données, pays colorisés suivant le nombre d'objets conserrvés\n",
    "data.plot(column='count', cmap=cmap, norm=norm,\n",
    "          edgecolor='black', linewidth=0.2,alpha=0.85, ax=ax)\n",
    "\n",
    "# représentation des frontières en noir (et des pays non producteurs en gris clair)\n",
    "world.plot(\n",
    "    ax=ax,\n",
    "    color=(\"lightgrey\"),\n",
    "    edgecolor=\"black\",\n",
    "    alpha=.05\n",
    ")\n",
    "\n",
    "# choix du domaine représenté (ici Europe uniquement)\n",
    "ax.set_xlim(-10, 32)\n",
    "ax.set_ylim(32, 70)\n",
    "ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Annotation textuelles\n",
    "# extraction de la position des centres des pays\n",
    "data_projected = data.to_crs(epsg=3035)\n",
    "data_projected['centroid'] = data_projected.geometry.centroid\n",
    "data['centroid'] = data_projected['centroid'].to_crs(data.crs)\n",
    "# liste des pays à annoter\n",
    "countries_to_annotate = ['France', 'Italy', 'Switzerland','Poland','Sweden','Denmark','Czechia',\n",
    "                         'Spain','Netherlands','Belgium', 'Germany','United Kingdom',\n",
    "                         'United States of America','Greece','Sweden','Austria','Georgia']\n",
    "\n",
    "# Ajustement manuel de la position des texte 'Nom du pays':(dx,dy) où dx et dy sont les variation horizontales et verticales de la position\n",
    "adjustments = {\n",
    "    'France': (10, 3),\n",
    "    'Italy': (-0.5, -0),\n",
    "    'Lithuania': (0, -0.6),\n",
    "    'Finland': (0, -2.5),\n",
    "    'Romania': (0, -0.5),\n",
    "    'Bulgaria': (0, -0.6),\n",
    "    'Greece': (-1.3, -0.),\n",
    "    'Croatia': (0, -1),\n",
    "    'Cyprus': (0, -1),\n",
    "    'Ireland': (0, -1),\n",
    "    'Malta': (0, -1),\n",
    "    'Slovenia': (0, -1),\n",
    "    'Slovakia': (-0.7, -0.8),\n",
    "    'Estonia': (0, -0.7),\n",
    "    'Latvia': (0, -0.5),\n",
    "    'Belgium': (-0.1, -0.5),\n",
    "    'Austria': (0.2, -0.5),\n",
    "    'Spain': (0, -1),\n",
    "    'Portugal': (-0.5, -1),\n",
    "    'Luxembourg': (0, -1),\n",
    "    'Germany': (-0.2, 0),\n",
    "    'Hungary': (-0.3, -1),\n",
    "    'Czechia': (-0.5, -0.5),\n",
    "    'Poland': (0, -1),\n",
    "    'Sweden': (-1.5, -1),\n",
    "    'Denmark': (-0.8, -0.1),\n",
    "    'Netherlands': (-0.2, -0.3),\n",
    "    'United Kingdom': (1, -2),\n",
    "    'Switzerland': (-0.2, -0.5),\n",
    "    'Turkey':(0, -1.),\n",
    "    'Iran':(0, 0),\n",
    "    'Egypt':(0, 0),\n",
    "    'United States of America':(0, 0),\n",
    "    'Sweden':(-1, 0),\n",
    "    'Georgia':(0, 0),\n",
    "}\n",
    "\n",
    "# annotation\n",
    "for country in countries_to_annotate:\n",
    "\n",
    "    # position du texte\n",
    "    centroid = data.loc[data['NAME'] == country, 'centroid'].values[0]\n",
    "    x, y = centroid.coords[0]\n",
    "\n",
    "    # corrections de la position\n",
    "    x += adjustments[country][0]\n",
    "    y += adjustments[country][1]\n",
    "\n",
    "    # texte\n",
    "    code = data.loc[data['NAME'] == country, 'SOV_A3'].values[0]\n",
    "    rate = data.loc[data['NAME'] == country, 'count'].values[0]\n",
    "    ax.annotate(f'{code[:2]} {rate}%', (x, y), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=10, color='black')\n",
    "\n",
    "# Annotation des pays extra-européens\n",
    "ax.annotate('Hors-carte :', (-5, 65), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=15,fontweight='bold', color='black')\n",
    "rateUSA = data.loc[data['NAME'] == 'United States of America', 'count'].values[0]\n",
    "ax.annotate('USA %1.1f' %rateUSA+'%', (-5, 64), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=15, color='black')\n",
    "\n",
    "rateGeorgia = data.loc[data['NAME'] == 'Georgia', 'count'].values[0]\n",
    "ax.annotate('Géorgie %1.1f' %rateGeorgia+'%', (-5, 63), textcoords=\"offset points\", xytext=(5, 5),\n",
    "                ha='center', fontsize=15, color='black')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pied-de-page\n",
    "text = \"<Auteure>: Aurelia Valterio\\n<Source>: https://www.naturalearthdata.com/\"\n",
    "fig_text(0.085, .1,\n",
    "         s=text,\n",
    "         color='black',\n",
    "         fontsize=9,\n",
    "         highlight_textprops=[{\"fontweight\": 'bold'},\n",
    "                              {\"fontweight\": 'bold'}],\n",
    "         ax=ax)\n",
    "\n",
    "# titre\n",
    "ax.set_title(\"Répartition géographique des lieux de conservation des objets\", fontsize=25,y=1.01) \n",
    "\n",
    "\n",
    "\n",
    "# sauvegarde de la figure\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figs/Geo_pays_cons.pdf',bbox_inches=\"tight\")\n",
    "# affichage de la figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7737c",
   "metadata": {},
   "source": [
    "## 4 Partitionnement non-supervisé d'un corpus d'images médiévales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin vers le dossier contenant les images\n",
    "path = '/Users/aureliavalterio/Desktop/Mémoire_HN/OK_VF/'\n",
    "\n",
    "# déplacement du dossier de travail dans le dossier des images\n",
    "os.chdir(path)\n",
    "\n",
    "\n",
    "# lecture du dossier et création de la liste\n",
    "images_to_cluster = os.listdir(path)\n",
    "\n",
    "# estimation du nombre d'image dans la liste \n",
    "# il se peut qu'un ou deux fichiers autres que des images se soient glissés dans la liste\n",
    "# ces dernier seront mis de côtés lors de l'analyse\n",
    "images_to_cluster[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle (VGG16) et définition d'une fonction pour extraire les charactéristiques des images avec keras\n",
    "\n",
    "model = VGG16()\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "\n",
    "def extract_features(file, model):\n",
    "    # chargement de l'image comme une matrice 224x224\n",
    "    img = load_img(file, target_size=(224,224))\n",
    "    # converstion de 'PIL.Image.Image' en un numpy array\n",
    "    img = np.array(img) \n",
    "    # redimensionnement des données pour le model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3) \n",
    "    # préparation des images pour le modèle\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    # création du vecteur avec les charactéristiques\n",
    "    features = model.predict(imgx)\n",
    "    return features\n",
    "\n",
    "# Test de la fonction extract_features\n",
    "\n",
    "extract_features(images_to_cluster[0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "p = '/Users/aureliavalterio/Desktop/Mémoire_HN/images_features.pkl'\n",
    "\n",
    "\n",
    "\n",
    "# boucle sur toutes les images du corpus\n",
    "for imgtc in images_to_cluster:\n",
    "    # extraction des characteristiques et mise à jour du dictionnaire\n",
    "    try:\n",
    "        feat = extract_features(imgtc,model)\n",
    "        data[imgtc] = feat\n",
    "    # si erreur, sauvegarde des characteristiques dans un fichier pickle (optionel)\n",
    "    except:\n",
    "        with open(p,'wb') as file:\n",
    "            pickle.dump(data,file)\n",
    "\n",
    "# création d'une liste avec les nom des fichiers\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# création d'une liste avec les charactéristiques\n",
    "feat = np.array(list(data.values()))\n",
    "print(feat.shape)\n",
    "\n",
    "\n",
    "# redimensionnement pour avoir xxx échantillons avec 4096 vecteurs\n",
    "feat = feat.reshape(-1,4096)\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228097b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction dimensionelle et partitionnement avec kmeans\n",
    "\n",
    "pca = PCA(n_components=100, random_state=22)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)\n",
    "\n",
    "# Choix du nombre de cluster\n",
    "n_clusters=5\n",
    "\n",
    "kmeans = KMeans(n_clusters, random_state=22)\n",
    "kmeans.fit(x)\n",
    "\n",
    "# Impression du nombre de cluster pour chaque image du corpus\n",
    "print(kmeans.labels_)\n",
    "print(len(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01865adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitionnement des images dans les clusters et définition d'une fonction permettant de voir les images dans chaque cluster\n",
    "\n",
    "groups = {}\n",
    "for file, cluster in zip(filenames,kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)\n",
    "        \n",
    "def view_cluster(cluster):\n",
    "    plt.figure(figsize = (25,25));\n",
    "    # liste des noms de fichiers dans le cluster \n",
    "    files = groups[cluster]\n",
    "    # limite de 100 images affichées\n",
    "    if len(files) > 26:\n",
    "        print(f\"Clipping cluster size from {len(files)} to 25\")\n",
    "        files = files[:25]\n",
    "    # affichage des images présentes dans le cluster\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(5,5,index+1);\n",
    "        img = load_img(file)\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.savefig('/Users/aureliavalterio/Desktop/Mémoire_HN/Figs/Cluster_'+str(cluster)+'.pdf',bbox_inches=\"tight\")\n",
    "\n",
    "        \n",
    "for i in range(n_clusters):\n",
    "    print('Cluster ',i+1)\n",
    "    view_cluster(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa5c9b",
   "metadata": {},
   "source": [
    "## 5 Reconnaissance faciale et partitionnement des visages\n",
    "\n",
    "### 5.1 Reconnaissance faciale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# haar cascade\n",
    "path = '/Users/aureliavalterio/Desktop/Mémoire_HN/OK_VF/'\n",
    "name = '394_Reliquaire_Buste du pape Léon 3_19_Aix la Chapelle_Allemagne_Aix la Chapelle_Allemagne_NaN.jpg'\n",
    "#image_path = path + '261_Reliquaire_NaN_12_NaN_NaN_Paris_France_NaN.jpg'\n",
    "#image_path = path + '14_Reliquaire_Althée_9_NaN_Allemagne_Sion_Suisse_NaN.jpg'\n",
    "image_path = path+name\n",
    "\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "face = face_classifier.detectMultiScale(\n",
    "    gray_image, scaleFactor=1.1, minNeighbors=8, minSize=(40, 40))\n",
    "\n",
    "\n",
    "\n",
    "index=1\n",
    "plt.subplot(1,len(face)+1,index);\n",
    "\n",
    "for (column, row, width, height) in face:\n",
    "    image = cv2.rectangle(img,(column, row),(column + width, row + height),(0, 255, 0),40)\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# Pour afficher\n",
    "for (column, row, width, height) in face:\n",
    "    plt.subplot(1,len(face)+1,index+1);\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_rec = image[row:row+width, column:column+width]\n",
    "#    print(image_rec)\n",
    "    plt.imshow(image_rec)\n",
    "    plt.axis(\"off\")\n",
    "    index+=1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62342c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/aureliavalterio/Desktop/Mémoire_HN/OK_VF/'\n",
    "name = '394_Reliquaire_Buste du pape Léon 3_19_Aix la Chapelle_Allemagne_Aix la Chapelle_Allemagne_NaN.jpg'\n",
    "#image_path = path + '261_Reliquaire_NaN_12_NaN_NaN_Paris_France_NaN.jpg'\n",
    "#image_path = path + '14_Reliquaire_Althée_9_NaN_Allemagne_Sion_Suisse_NaN.jpg'\n",
    "image_path = path+name\n",
    "\n",
    "\n",
    "faces = RetinaFace.extract_faces(image_path,\n",
    "                                 threshold=0.8,align = True)\n",
    "\n",
    "if len(faces)>0:\n",
    "    fig, ax = plt.subplots(1,len(faces)+1,figsize=(15,10))\n",
    "    image = mpimg.imread(image_path)\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_axis_off()   \n",
    "    \n",
    "    index=1\n",
    "    for face in faces:\n",
    "        ax[index].imshow(face)\n",
    "        ax[index].set_axis_off()\n",
    "        index+=1\n",
    "    plt.show()\n",
    "    index=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/aureliavalterio/Desktop/Mémoire_HN/OK_VF/'\n",
    "\n",
    "\n",
    "# Pour éviter les doublons : supprimer le dossier s'il existe\n",
    "newpath = '/Users/aureliavalterio/Desktop/Mémoire_HN/Detected_faces/'\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "dossier = os.listdir(path)\n",
    "ext = [\".jpeg\", \".png\", \".jpg\"]\n",
    "\n",
    "count=0\n",
    "for files in dossier:\n",
    "    if files.endswith(tuple(ext)):\n",
    "        try:\n",
    "            count+=1\n",
    "            image_path = path+files\n",
    "            faces = RetinaFace.extract_faces(image_path,threshold=0.8,align = True)\n",
    "            if len(faces)>0:\n",
    "                print(count,files)\n",
    "                fig, ax = plt.subplots(1,len(faces)+1)\n",
    "                image = mpimg.imread(image_path)\n",
    "                ax[0].imshow(image)\n",
    "                ax[0].set_axis_off()   \n",
    "                index=1\n",
    "                for face in faces:\n",
    "                    ax[index].imshow(face)\n",
    "                    ax[index].set_axis_off()\n",
    "                    index+=1\n",
    "            plt.show()\n",
    "            index=1\n",
    "            for face in faces:\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.imshow(face)\n",
    "                ax.set_axis_off()\n",
    "                plt.savefig(newpath+files.split('.')[0]+'_'+str(index)+'.jpg',bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "                index+=1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af364f4d",
   "metadata": {},
   "source": [
    "### 5.2 Clustering des visages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298277e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour le partionnement des images\n",
    "# chargement et analyse des images  \n",
    "from keras.preprocessing.image import load_img \n",
    "from keras.preprocessing.image import img_to_array \n",
    "from keras.applications.vgg16 import preprocess_input \n",
    "\n",
    "# models \n",
    "from keras.applications.vgg16 import VGG16 \n",
    "from keras.models import Model\n",
    "\n",
    "# partitionnement et reduction dimensionelle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin vers le dossier contenant les images\n",
    "path = '/Users/aureliavalterio/Desktop/Mémoire_HN/Detected_faces/'\n",
    "\n",
    "# changement du dossier de travail dans le dossier où se trouvent les images\n",
    "os.chdir(path)\n",
    "\n",
    "# lecture du dossier et création de la liste\n",
    "images_to_cluster = os.listdir(path)\n",
    "\n",
    "# estimation du nombre d'image dans la liste \n",
    "# il se peut qu'un ou deux fichiers autres que des images se soient glissés dans la liste\n",
    "# ces dernier seront mis de côtés lors de l'analyse\n",
    "\n",
    "print(images_to_cluster[:10])\n",
    "print('Le nombre d\\'images est',len(images_to_cluster),'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860cbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle (VGG16) et définition d'une fonction pour extraire les charactéristiques des images avec keras\n",
    "\n",
    "model = VGG16()\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "\n",
    "def extract_features(file, model):\n",
    "    # chargement de l'image comme une matrice 224x224\n",
    "    img = load_img(file, target_size=(224,224))\n",
    "    # converstion de 'PIL.Image.Image' en un numpy array\n",
    "    img = np.array(img) \n",
    "    # redimensionnement des données pour le model reshape(num_of_samples, dim 1, dim 2, channels)\n",
    "    reshaped_img = img.reshape(1,224,224,3) \n",
    "    # préparation des images pour le modèle\n",
    "    imgx = preprocess_input(reshaped_img)\n",
    "    # création du vecteur avec les charactéristiques\n",
    "    features = model.predict(imgx)\n",
    "    return features\n",
    "\n",
    "# Test de la fonction extract_features\n",
    "\n",
    "extract_features(images_to_cluster[0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc50707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "p = '/Users/aureliavalterio/Desktop/Mémoire_HN/images_features.pkl'\n",
    "\n",
    "\n",
    "\n",
    "# lop through each image in the dataset\n",
    "for imgtc in images_to_cluster:\n",
    "    # try to extract the features and update the dictionary\n",
    "    try:\n",
    "        feat = extract_features(imgtc,model)\n",
    "        data[imgtc] = feat\n",
    "    # if something fails, save the extracted features as a pickle file (optional)\n",
    "    except:\n",
    "        with open(p,'wb') as file:\n",
    "            pickle.dump(data,file)\n",
    "          \n",
    " \n",
    "# get a list of the filenames\n",
    "filenames = np.array(list(data.keys()))\n",
    "\n",
    "# get a list of just the features\n",
    "feat = np.array(list(data.values()))\n",
    "print(feat.shape)\n",
    "\n",
    "\n",
    "# reshape so that there are xxx samples of 4096 vectors\n",
    "feat = feat.reshape(-1,4096)\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensional reduction and clustering using kmeans\n",
    "\n",
    "pca = PCA(n_components=100, random_state=22)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)\n",
    "\n",
    "# ENTER HERE THE NUMBER OF CLUSTERS\n",
    "n_clusters=6\n",
    "\n",
    "kmeans = KMeans(n_clusters, random_state=22)\n",
    "kmeans.fit(x)\n",
    "\n",
    "# Print the cluster label of each image\n",
    "print(kmeans.labels_)\n",
    "print(len(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2c370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# partitionnement des images dans les clusters et définition d'une fonction permettant de voir les images dans chaque cluster\n",
    "\n",
    "groups = {}\n",
    "for file, cluster in zip(filenames,kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)\n",
    "        \n",
    "def view_cluster(cluster):\n",
    "    plt.figure(figsize = (25,25));\n",
    "    # liste des noms de fichiers dans le cluster \n",
    "    files = groups[cluster]\n",
    "    # limite de 100 images affichées\n",
    "    if len(files) > 25:\n",
    "        print(f\"Clipping cluster size from {len(files)} to 25\")\n",
    "        files = files[:25]\n",
    "    # affichage des images présentes dans le cluster\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(5,5,index+1);\n",
    "        img = load_img(file)\n",
    "        img = np.array(img)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.savefig('/Users/aureliavalterio/Desktop/Mémoire_HN/Figs/Faces_Cluster_'+str(cluster)+'.pdf',bbox_inches=\"tight\")\n",
    "\n",
    "        \n",
    "for i in range(n_clusters):\n",
    "    print('Cluster ',i+1)\n",
    "    view_cluster(i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd923fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
